#################################################

# Play specific settings
#################################################

# The secret used for cryptography
play.crypto.secret = "QCY?tAnfk?aZ?iwKNwnxIlROaK6hiquoo2EvohyuihB`R5W:(/uDFN];Ik@n"

# The application loader
play.application.loader = "EccencaApplicationLoader"

# set application context
play.http.context= ${SERVER_CONTEXTPATH}

# set application max upload file size
play.http.parser.maxDiskBuffer=1024MB

#################################################
# eccenca DataPlatform
#################################################

# The URL of the eccenca DataPlatform
eccencaDataPlatform.url = ${DEPLOY_BASE_URL}"/dataplatform"
eccencaDataPlatform.endpointId = "default"
eccencaDataPlatform.oauth = true
oauth.clientId = "eldsClient"
oauth.clientSecret = "secret"
oauth.authorizationUrl = ${eccencaDataPlatform.url}"/oauth/authorize"
oauth.tokenUrl = ${eccencaDataPlatform.url}"/oauth/token"

# Define the protocol used for accessing the workbench (http or https), defaults to http
workbench.protocol = ${DEPLOY_PROTOCOL}

#################################################
# Workspace
#################################################


# Set the used workspace
workspace.provider.plugin = backend

# Holds all resources on the HDFS file system
#workspace.repository.plugin = hdfs
workspace.repository.plugin = file

workspace.repository.file = {
  dir = "/data/datalake"
}
workspace.repository.hdfs {
  path = "hdfs://hadoop:9000/data/",
  user = "hadoopuser"
}



# hide header
workbench.showHeader=false

# Workspace on the RDF store (requires backend.url parameter to be defined)
workspace.provider.backend = {
  dir = "/data/" # Optional: File system path for holding resources, such as input datasets
}

workflow.executor.plugin = ExecuteLocalWorkflow

#################################################
# Internal Dataset
#################################################

dataset.internal.plugin = inMemory
# dataset.internal.plugin = eccencaDataPlatform
# dataset.internal.plugin = sparqlEndpoint

dataset.internal.eccencaDataPlatform = {
  graph = "https://ns.eccenca.com/dataintegration/internal"
}

#################################################
# Provenance enabled
#################################################

user.manager.web.plugin=oauthUserManager
provenance.persistWorkflowProvenancePlugin.plugin=rdfWorkflowProvenance


#################################################
# Spark
#################################################
spark.interpreter.options = {
  # Generic comma separated properties, optional
  sparkConfiguration = ""
  # URL of the Spark cluster master node, optional, default is local[1]
  sparkMaster = "local[4]"
  # The IP of the driver programms machine in client-mode, required for client and cluster modes
  sparkLocalIP = ""
  # jars containing the spark code, local file will be copied to the cluster, required for client mode
  sparkJars = "/opt/elds/dataintegration/dist/spark-assembly.jar"
  #sparkJars = "/opt/elds/dataintegration/dist/etc/dataintegration/eccenca-DataIntegration-assembly.jar"
  # Specifies local, client-mode or full cluster-mode deployment, required: local,client or cluster
  deploymentMode = "local"
  # Folder for intermediate results, optional, default: /tmp
  temporaryFolder = "/tmp/"
  # Specifies the folder for event and time log, optional, default: /tmp
  logFolder = "/tmp/"
  # Persists intermediate results from all phases (DataSet load/store, Transform, Linking) if true
  materializeIntermediateResults = false
  # Persists all data sets in Hive if true (concurrently to saving them in the specified data set)
  materializeInHive = false
  # Enables more detailed logging, counting of transformed records, etc.
  debugMode = false
  # Enables the use of certain testing implementations for non-integration tests (e.g. HiveTestContext instead of HiveContext)
  testMode = false
  # Enable or disable a spark executor event log for debugging purposes
  eventLog = true
  # Enable or disable an execution time log for benchmarking purposes
  timeLog = false
  # Enable or disable Sparks built-in log for debugging purposes
  sparkLog = true
  # If true, data will be repartitioned before execution, otherwise the existing partitioning or no partitioning will be used
  partitionOnImport = false
  # Number of partitions for repartitioning on Import (for local filesystems and local execution modes a low number is recommended)
  partitionNumber = 4
  # Attribute used for Repartioning
  partitionKey = "URI"
  # Specifies number of Spark SQL shuffle partitions
  shufflePartitions = 4
  # Minimum partition number for Spark execution
  defaultMinPartitions = 4
  # Default parallelism partition number for Spark execution
  defaultParallelism = 4
  # Specifies if data is combined before output is written to disk. If true the final output will be in a single file on a single partition
  # This needs to be true when working with resources on the local filesystem (e.g. when csv file is used a target dataset of a workflow)
  combineOutput = true
}

###############################################
# External matching project
###############################################

# The external matching project in the workspace containing the matching link spec
matching.external.projectId = "matchingProject"
# The link spec in the matching project that should be executed on the source and target ontology
matching.external.linkSpecId = "matchingLinkSpec"


#################################################
# Logging
#################################################

# Logging Configuration
logger.root=DEBUG
logger.play=INFO
logger.application=DEBUG
